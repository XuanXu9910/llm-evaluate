{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from rich import print as pprint\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ResponseRelevancy, Faithfulness, FactualCorrectness, SemanticSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input 為問題\n",
    "# reference 為標準答案\n",
    "# response 為RAG應用所提供的回答\n",
    "# retrieved_contexts 為RAG所檢索的文檔\n",
    "\n",
    "csv_path = \"C:/NTUST/Research/llm-evaluate/eval/eval_global_4omini_USI.csv\"\n",
    "file_name = os.path.splitext(os.path.basename(csv_path))[0]       # 提取檔名（不包含路徑和副檔名）\n",
    "method_name = file_name.replace(\"eval_\", \"\")                      # 移除 \"eval_\" 的部分\n",
    "\n",
    "dataset_df = pd.read_csv(csv_path)\n",
    "dataset_df[\"retrieved_contexts\"] = dataset_df[\"retrieved_contexts\"].apply(json.loads)\n",
    "dataset_df = dataset_df[['user_input', 'reference', 'response', 'retrieved_contexts']]\n",
    "eval_dataset = EvaluationDataset.from_pandas(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    model=os.environ[\"AZURE_OPENAI_CHAT_MODEL_NAME\"],\n",
    "    validate_base_url=False,\n",
    "))\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_EMBED_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT_NAME\"],\n",
    "    model=os.environ[\"AZURE_OPENAI_EMBED_MODEL_NAME\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 120/120 [02:51<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm), \n",
    "    ContextEntityRecall(llm=evaluator_llm),\n",
    "    # NoiseSensitivity(llm=evaluator_llm),\n",
    "    ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)\n",
    "df_result = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[\"retrieved_contexts\"] = df_result[\"retrieved_contexts\"].apply(json.dumps)\n",
    "df_result.to_csv(f\"./result/result_{method_name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEvaluate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
