{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from rich import print as pprint\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input 為問題\n",
    "# reference 為標準答案\n",
    "# response 為RAG應用所提供的回答\n",
    "# retrieved_contexts 為RAG所檢索的文檔\n",
    "\n",
    "dataset_df = pd.read_csv(\"./evalData_Asia_Cement_Corporation.csv\")\n",
    "dataset_df[\"retrieved_contexts\"] = dataset_df[\"retrieved_contexts\"].apply(json.loads)\n",
    "dataset_df = dataset_df[['user_input', 'reference', 'response', 'retrieved_contexts']]\n",
    "eval_dataset = EvaluationDataset.from_pandas(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    model=os.environ[\"AZURE_OPENAI_CHAT_MODEL_NAME\"],\n",
    "    validate_base_url=False,\n",
    "))\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_EMBED_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT_NAME\"],\n",
    "    model=os.environ[\"AZURE_OPENAI_EMBED_MODEL_NAME\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 16/16 [00:46<00:00,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm), \n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What frameworks does the ESG report follow, an...</td>\n",
       "      <td>[-----Entities-----\\nid|entity|description|num...</td>\n",
       "      <td>## Overview of ESG Reporting Frameworks\\n\\nThe...</td>\n",
       "      <td>## Frameworks Followed in the ESG Report\\n\\nTh...</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is the organizational scope and time frame of ...</td>\n",
       "      <td>[-----Entities-----\\nid|entity|description|num...</td>\n",
       "      <td>### Organizational Scope and Time Frame of the...</td>\n",
       "      <td>## Organizational Scope and Time Frame of the ...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.757469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the report link the company’s ESG ini...</td>\n",
       "      <td>[-----Entities-----\\nid|entity|description|num...</td>\n",
       "      <td>## Linking ESG Initiatives to Business Strateg...</td>\n",
       "      <td>## Linking ESG Initiatives to Business Strateg...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the report compare current ESG performanc...</td>\n",
       "      <td>[-----Entities-----\\nid|entity|description|num...</td>\n",
       "      <td>### ESG Performance Data Comparisons\\n\\nThe cu...</td>\n",
       "      <td>## Comparison of ESG Performance with Historic...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.872454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What frameworks does the ESG report follow, an...   \n",
       "1  Is the organizational scope and time frame of ...   \n",
       "2  How does the report link the company’s ESG ini...   \n",
       "3  Does the report compare current ESG performanc...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [-----Entities-----\\nid|entity|description|num...   \n",
       "1  [-----Entities-----\\nid|entity|description|num...   \n",
       "2  [-----Entities-----\\nid|entity|description|num...   \n",
       "3  [-----Entities-----\\nid|entity|description|num...   \n",
       "\n",
       "                                            response  \\\n",
       "0  ## Overview of ESG Reporting Frameworks\\n\\nThe...   \n",
       "1  ### Organizational Scope and Time Frame of the...   \n",
       "2  ## Linking ESG Initiatives to Business Strateg...   \n",
       "3  ### ESG Performance Data Comparisons\\n\\nThe cu...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  ## Frameworks Followed in the ESG Report\\n\\nTh...        0.722222   \n",
       "1  ## Organizational Scope and Time Frame of the ...        0.636364   \n",
       "2  ## Linking ESG Initiatives to Business Strateg...        0.882353   \n",
       "3  ## Comparison of ESG Performance with Historic...        1.000000   \n",
       "\n",
       "   factual_correctness  faithfulness  semantic_similarity  \n",
       "0                 0.29      1.000000             0.899229  \n",
       "1                 0.31      0.250000             0.757469  \n",
       "2                 0.37      1.000000             0.815731  \n",
       "3                 0.52      0.217391             0.872454  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result = results.to_pandas()\n",
    "eval_result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEvaluate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
